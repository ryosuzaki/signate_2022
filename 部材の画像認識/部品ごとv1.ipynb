{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2091,"status":"ok","timestamp":1657544469747,"user":{"displayName":"須崎涼","userId":"17559993123309156265"},"user_tz":-540},"id":"HTqLuwxVmP24","outputId":"07994df8-d973-48e5-cbf6-4475ad62577e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# 新しいセクション"],"metadata":{"id":"HbebBuSdPKpt"}},{"cell_type":"code","source":["from skimage import io\n","import torch\n","from torch import nn\n","import torchvision.transforms as transforms\n","from PIL import Image\n","\n","input=\"/content/drive/MyDrive/signate/タナチョー 部材の画像認識/train/000/0.jpg\"\n","image = io.imread(input)\n","\n","class ImageTransform():\n","  def __init__(self):\n","    self.data_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","  def __call__(self, img):\n","    return self.data_transform(img)\n","\n","IT=ImageTransform()\n","tensor_img=IT(Image.fromarray(image))\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","tensor_img=tensor_img.unsqueeze(0).to(device)"],"metadata":{"id":"ZG-CNGBiWVSV","executionInfo":{"status":"ok","timestamp":1657546408604,"user_tz":-540,"elapsed":637,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n","model.classifier=nn.Linear(1024, 16,bias=True)\n","model.to(device)\n","load_path =\"/content/drive/MyDrive/signate/タナチョー 部材の画像認識/test/category_color_model20220709_205149.pth\"\n","if torch.cuda.is_available():\n","    model.load_state_dict(torch.load(load_path))\n","else:\n","    model.load_state_dict(torch.load(load_path, map_location=torch.device('cpu')))\n","model = model.eval()\n","category_color_model_output=model(tensor_img)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n","model.classifier=nn.Linear(1024, 122,bias=True)\n","model.to(device)\n","load_path = \"/content/drive/MyDrive/signate/タナチョー 部材の画像認識/test/model20220706_204326.pth\"\n","if torch.cuda.is_available():\n","    model.load_state_dict(torch.load(load_path))\n","else:\n","    model.load_state_dict(torch.load(load_path, map_location=torch.device('cpu')))\n","model = model.eval()\n","model_output=model(tensor_img)\n","print(category_color_model_output,model_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HI8ck7HUaeEZ","executionInfo":{"status":"ok","timestamp":1657546414295,"user_tz":-540,"elapsed":1959,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"a1dbabfe-7bc1-4655-fa8c-a1b4cf6a2507"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[ 1.2790, -0.9569,  0.6175, -0.6302, -0.6282, -1.2318, -0.3290,  2.8677,\n","          2.9647, -0.3625, -1.0875, -2.6111, -0.7639, -0.1415,  1.1913, -0.9498]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>) tensor([[ 7.2688e+00,  1.9152e+00, -3.2463e-01,  1.0300e+00, -2.7080e-01,\n","          6.8408e-01, -2.0005e-01,  1.8899e+00, -1.0493e+00,  1.7427e-01,\n","         -3.0288e-01,  2.4924e+00, -3.9594e-01,  3.3056e-01, -1.9115e+00,\n","          6.9823e+00, -8.0050e-01, -2.9430e-01, -1.1936e+00, -2.5053e-01,\n","         -4.3599e-01, -7.9199e-01,  6.9291e-01, -4.5397e-01,  1.9666e-01,\n","         -1.7488e-01,  1.6169e+00, -2.1142e-01,  1.5380e+00, -1.1996e+00,\n","          6.0954e-01,  1.6594e+00,  2.5842e+00, -1.9933e-01,  9.6935e-01,\n","          1.6403e-01,  1.2224e+00, -1.7831e-01, -7.8017e-01, -6.5535e-01,\n","          7.8778e-01,  1.6146e+00, -8.1748e-02,  1.7029e+00,  7.0823e-02,\n","          1.5530e-01, -5.0977e-03,  2.9723e-01,  4.0680e-01,  2.9617e+00,\n","         -1.0981e+00, -4.6410e-01, -3.8109e-01, -1.5556e+00, -1.3572e+00,\n","          3.3300e+00,  6.2499e-01,  6.2770e-01, -3.0454e-01,  3.7328e-01,\n","         -1.6339e-01, -3.4039e-01,  3.4358e-01,  1.7784e-01, -9.1531e-01,\n","          1.2046e+00,  6.4411e-01, -1.2353e-01,  3.7343e-01,  5.4497e-01,\n","          8.6752e-02, -2.9677e-01, -8.1378e-01, -2.3095e+00, -1.0008e+00,\n","         -4.4141e-01,  2.1908e+00, -2.4545e-01,  3.1521e+00,  3.5100e-01,\n","          1.9455e+00, -6.3264e-01,  4.8758e-01, -2.7402e-01,  7.4494e-01,\n","          3.3325e+00, -1.8934e+00, -1.3590e+00, -4.7395e-01,  2.4515e+00,\n","         -1.0805e+00,  2.4837e-01, -1.9007e-01, -5.7589e-01,  5.3002e-01,\n","         -1.0936e+00,  7.6048e-01,  3.3463e-01, -3.8270e-01,  1.2991e+00,\n","         -1.0485e+00, -2.6693e+00, -2.7208e+00, -2.5206e+00, -1.9877e+00,\n","         -2.6612e+00, -2.7558e+00, -2.6335e+00, -2.0359e+00, -1.6710e+00,\n","         -2.4584e+00, -1.9785e+00, -2.5417e+00, -1.6579e+00, -2.4934e+00,\n","         -1.7422e+00, -2.0635e+00, -2.0121e+00, -2.6054e+00, -1.8989e+00,\n","         -2.4199e+00, -2.1122e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["category_color_model_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Is0GLFRd4gW","executionInfo":{"status":"ok","timestamp":1657546448953,"user_tz":-540,"elapsed":383,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"20314694-0397-4458-f835-f22b212c4781"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.2790, -0.9569,  0.6175, -0.6302, -0.6282, -1.2318, -0.3290,  2.8677,\n","          2.9647, -0.3625, -1.0875, -2.6111, -0.7639, -0.1415,  1.1913, -0.9498]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"MlWK5TJkfcHe"},"source":["# データ処理"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":638,"status":"ok","timestamp":1657546924665,"user":{"displayName":"須崎涼","userId":"17559993123309156265"},"user_tz":-540},"id":"iKefI9ikV8ao","outputId":"22b45d68-1ad4-44cf-de50-4ff29a721751"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["    category color category_color  encoded_category  encoded_color  \\\n","0      クレセント    YW        クレセントYW                 0             10   \n","1      クレセント    A3        クレセントA3                 0              0   \n","2      クレセント    DG        クレセントDG                 0              3   \n","3      クレセント    YS        クレセントYS                 0              9   \n","4         戸車    YS           戸車YS                 1              9   \n","..       ...   ...            ...               ...            ...   \n","117       戸車    YS           戸車YS                 1              9   \n","118       戸車    YS           戸車YS                 1              9   \n","119       戸車    YS           戸車YS                 1              9   \n","120    クレセント    WM        クレセントWM                 0              6   \n","121       戸車    YS           戸車YS                 1              9   \n","\n","     encoded_category_color  \n","0                         8  \n","1                         0  \n","2                         3  \n","3                         7  \n","4                        14  \n","..                      ...  \n","117                      14  \n","118                      14  \n","119                      14  \n","120                       4  \n","121                      14  \n","\n","[122 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-94939b9f-883a-4779-b7a3-621fbd80b476\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>category</th>\n","      <th>color</th>\n","      <th>category_color</th>\n","      <th>encoded_category</th>\n","      <th>encoded_color</th>\n","      <th>encoded_category_color</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>クレセント</td>\n","      <td>YW</td>\n","      <td>クレセントYW</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>クレセント</td>\n","      <td>A3</td>\n","      <td>クレセントA3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>クレセント</td>\n","      <td>DG</td>\n","      <td>クレセントDG</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>クレセント</td>\n","      <td>YS</td>\n","      <td>クレセントYS</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>戸車</td>\n","      <td>YS</td>\n","      <td>戸車YS</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>117</th>\n","      <td>戸車</td>\n","      <td>YS</td>\n","      <td>戸車YS</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>118</th>\n","      <td>戸車</td>\n","      <td>YS</td>\n","      <td>戸車YS</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>119</th>\n","      <td>戸車</td>\n","      <td>YS</td>\n","      <td>戸車YS</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>120</th>\n","      <td>クレセント</td>\n","      <td>WM</td>\n","      <td>クレセントWM</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>121</th>\n","      <td>戸車</td>\n","      <td>YS</td>\n","      <td>戸車YS</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>14</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>122 rows × 6 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94939b9f-883a-4779-b7a3-621fbd80b476')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-94939b9f-883a-4779-b7a3-621fbd80b476 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-94939b9f-883a-4779-b7a3-621fbd80b476');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":24}],"source":["import pandas as pd\n","import json\n","\n","df=pd.read_json('/content/drive/MyDrive/signate/タナチョー 部材の画像認識/train_meta.json').T\n","df[\"category_color\"]=df.loc[:,\"category\"]+df.loc[:,\"color\"]\n","\n","def encode_df_label(df,col:str):\n","  from sklearn.preprocessing import LabelEncoder\n","  le=LabelEncoder()\n","  df['encoded_'+col]=le.fit_transform(df[col].values)\n","  return df\n","\n","df=encode_df_label(df,\"category\")\n","df=encode_df_label(df,\"color\")\n","df=encode_df_label(df,\"category_color\")\n","df"]},{"cell_type":"code","source":["df['encoded_category_color'].to_csv('/content/drive/MyDrive/signate/タナチョー 部材の画像認識/category_color.csv')"],"metadata":{"id":"Vwflu4t7fvEe","executionInfo":{"status":"ok","timestamp":1657547472320,"user_tz":-540,"elapsed":329,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["category_color=df['encoded_category_color'].to_list()"],"metadata":{"id":"5ebomYZJiqll","executionInfo":{"status":"ok","timestamp":1657547895732,"user_tz":-540,"elapsed":254,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["category_color_model_output,model_output=category_color_model_output.tolist(),model_output.tolist()\n","tmp=[0]*122\n","for i in range(len(category_color)):\n","    tmp[i]=category_color_model_output[0][category_color[i]]\n","category_color_model_output=tmp\n"],"metadata":{"id":"qNCURwlwitxD","executionInfo":{"status":"ok","timestamp":1657548659232,"user_tz":-540,"elapsed":221,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","added=np.array(category_color_model_output)+np.array(model_output)"],"metadata":{"id":"d-2kCUZckdtP","executionInfo":{"status":"ok","timestamp":1657549032508,"user_tz":-540,"elapsed":240,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["np.argsort(added[0])[::-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RVUlTv9Kmv1H","executionInfo":{"status":"ok","timestamp":1657549034263,"user_tz":-540,"elapsed":431,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"ff82959c-713d-4110-f00b-e3120b1e277f"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  0,  15,  85,  49,  11,  89,  76,  80,   7,  55,  31,  41,  78,\n","        99,  65,   3,  32,  96,  22,  56,  30,  59,  79,  97,   1,   9,\n","        35,  44,  26,  92,  33,  93,  40,  84,   5,  57,  69,  94,  82,\n","        48,  34,  62,  13,  91,  24,  63,  70,  45,  42,  46,  28,  43,\n","        67,  19,  60,  25,  37,   6,  68,  27,  77,   4,  47,  17,  10,\n","        58, 104,  23,  98,  12,  20,  51,  38,  16,  36,  95,  87,  53,\n","        61,  66, 119,  52, 111, 117, 108,  83, 121,   2,  39, 113,  75,\n","        88,  72,  81,  74,  71, 118, 116, 105, 101, 102,  64,  21, 100,\n","         8,  90,  18,  50, 115, 112,  29,  54,  86, 109, 114,  73,  14,\n","       120, 110, 107, 106, 103])"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["df.loc[:,\"encoded_category_color\"].max()+1"],"metadata":{"id":"EfFSfI5P0USf","executionInfo":{"status":"ok","timestamp":1657535550147,"user_tz":-540,"elapsed":272,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"554740d2-8c59-4324-bc01-86507f733139","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xjth6MD5LacM"},"outputs":[],"source":["import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import Subset\n","\n","#https://pytorch.org/vision/0.12/_modules/torchvision/datasets/folder.html\n","#https://pystyle.info/pytorch-how-to-create-custom-dataset-class/\n","class MyImageFolder(datasets.ImageFolder):\n","    # 要素を参照すると呼ばれる関数 \n","    def __init__(self,root,transform,label_df):\n","        super().__init__(root,transform)\n","        self.label_df=label_df\n","    def __getitem__(self, idx):\n","      sample, target=super().__getitem__(idx)\n","      return sample, self.label_df.iloc[target]\n","\n","\n","#https://pystyle.info/pytorch-list-of-transforms/\n","class ImageTransform():\n","  def __init__(self):\n","    self.data_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.RandomRotation(degrees=180),\n","        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n","        transforms.RandomPerspective(distortion_scale=0.2, p=0.9),\n","        #transforms.GaussianBlur(kernel_size=3),\n","        transforms.ToTensor(),\n","        #transforms.RandomErasing(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","  def __call__(self, img):\n","    return self.data_transform(img)"]},{"cell_type":"code","source":["def train_step(train_X, train_y):\n","    train_X, train_y=train_X.to(device),train_y.to(device)\n","\n","    # 訓練モードに設定\n","    model.train()\n","\n","    # フォワードプロパゲーションで出力結果を取得\n","    #train_X                # 入力データ\n","    pred_y = model(train_X) # 出力結果\n","    #train_y                # 正解ラベル\n","\n","    # 出力結果と正解ラベルから損失を計算し、勾配を求める\n","    optimizer.zero_grad()   # 勾配を0で初期化（※累積してしまうため要注意）\n","    loss = criterion(pred_y, train_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n","    loss.backward()   # 逆伝播の処理として勾配を計算（自動微分）\n","\n","    # 勾配を使ってパラメーター（重みとバイアス）を更新\n","    optimizer.step()  # 指定されたデータ分の最適化を実施 \n","\n","    # 正解率を算出\n","    with torch.no_grad(): # 勾配は計算しないモードにする\n","        acc=0\n","        for i in range(len(train_y)):\n","          acc+=1 if torch.argmax(pred_y[i])==train_y[i] else 0\n","\n","    # 損失と正解数をタプルで返す\n","    return (loss.item(), acc)  # ※item()=Pythonの数値\n","\n","def valid_step(valid_X, valid_y):\n","    valid_X, valid_y=valid_X.to(device),valid_y.to(device)\n","\n","    # 評価モードに設定（※dropoutなどの挙動が評価用になる）\n","    model.eval()\n","    \n","    # フォワードプロパゲーションで出力結果を取得\n","    #valid_X                # 入力データ\n","    pred_y = model(valid_X) # 出力結果\n","    #valid_y                # 正解ラベル\n","\n","    # 出力結果と正解ラベルから損失を計算\n","    loss = criterion(pred_y, valid_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n","\n","    # 正解率を算出\n","    with torch.no_grad(): # 勾配は計算しないモードにする\n","        acc=0\n","        for i in range(len(valid_y)):\n","          acc+=1 if torch.argmax(pred_y[i])==valid_y[i] else 0\n","\n","    # 損失と正解数をタプルで返す\n","    return (loss.item(), acc)  # ※item()=Pythonの数値"],"metadata":{"id":"eqzk4EEc3q-j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 部材種類色分類モデル"],"metadata":{"id":"eX_Zpl3GrXEI"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n","model.classifier=nn.Linear(1024, df.loc[:,\"encoded_category_color\"].max()+1,bias=True)\n","model.to(device)"],"metadata":{"id":"fYzpTJ3JrcO_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#https://tzmi.hatenablog.com/entry/2020/03/05/222813\n","load_path = '/content/drive/MyDrive/signate/タナチョー 部材の画像認識/model/category_color/model20220708_134723.pth'\n","if torch.cuda.is_available():\n","  model.load_state_dict(torch.load(load_path))\n","else:\n","  model.load_state_dict(torch.load(load_path, map_location=torch.device('cpu')))"],"metadata":{"id":"uVkrU2hKr1kG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim   # 「最適化」モジュールの別名定義\n","\n","# 定数（学習方法設計時に必要となるもの）\n","LEARNING_RATE = 1e-3   # 学習率： 0.03\n","REGULARIZATION = 0.03  # 正則化率： 0.03\n","\n","# オプティマイザーを作成（パラメーターと学習率も指定）\n","optimizer = optim.SGD(           # 最適化アルゴリズムに「SGD」を選択\n","    model.parameters(),          # 最適化で更新対象のパラメーター（重みやバイアス）\n","    lr=LEARNING_RATE,            # 更新時の学習率\n","    weight_decay=REGULARIZATION) # L2正則化（※不要な場合は0か省略）\n","\n","# 変数（学習方法設計時に必要となるもの）\n","criterion = nn.CrossEntropyLoss()  # 損失関数\n","\n","\n","\n","images=MyImageFolder(\"/content/drive/MyDrive/signate/タナチョー 部材の画像認識/train\",ImageTransform(),df.loc[:,\"encoded_category_color\"])\n","\n","batch_size = 64\n","images_size=len(images)\n","train_size = int((images_size * 0.8)//1)\n","dataset_train = Subset(images,list(range(0,train_size)))\n","dataset_valid   = Subset(images, list(range(train_size,images_size)))\n","print(\"train_len:\",len(dataset_train),\" vaild_len:\",len(dataset_valid))\n","\n","loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n","loader_valid = DataLoader(dataset_valid, batch_size=batch_size)\n","print(\"train_iter:\",len(loader_train),\" vaild_iter:\",len(loader_valid))\n","\n","\n","from datetime import datetime\n","from pytz import timezone\n","save_path='/content/drive/MyDrive/signate/タナチョー 部材の画像認識/model/category_color/model'+datetime.now(timezone('Asia/Tokyo')).strftime('%Y%m%d_%H%M%S')+'.pth'\n","\n","\n","\n","# 定数（学習／評価時に必要となるもの）\n","EPOCHS = 100         # エポック数： 100\n","\n","# 変数（学習／評価時に必要となるもの）\n","avg_loss = 0.0           # 「訓練」用の平均「損失値」\n","avg_acc = 0.0            # 「訓練」用の平均「正解率」\n","avg_val_loss = 0.0       # 「評価」用の平均「損失値」\n","avg_val_acc = 0.0        # 「評価」用の平均「正解率」\n","\n","# 損失の履歴を保存するための変数\n","train_history = []\n","valid_history = []\n","\n","print(\"Start Training At \"+datetime.now(timezone('Asia/Tokyo')).strftime('%m/%d %H:%M:%S'))\n","\n","for epoch in range(EPOCHS):\n","    # forループ内で使う変数と、エポックごとの値リセット\n","    total_loss = 0.0     # 「訓練」時における累計「損失値」\n","    total_acc = 0.0      # 「訓練」時における累計「正解数」\n","    total_val_loss = 0.0 # 「評価」時における累計「損失値」\n","    total_val_acc = 0.0  # 「評価」時における累計「正解数」\n","    total_train = 0      # 「訓練」時における累計「データ数」\n","    total_valid = 0      # 「評価」時における累計「データ数」\n","\n","    for train_X, train_y in loader_train:\n","        # 【重要】1ミニバッチ分の「訓練」を実行\n","        loss,acc = train_step(train_X, train_y)\n","\n","        # 取得した損失値と正解率を累計値側に足していく\n","        total_loss += loss          # 訓練用の累計損失値\n","        total_acc += acc            # 訓練用の累計正解数\n","        total_train += len(train_y) # 訓練データの累計数\n","            \n","    for valid_X, valid_y in loader_valid:\n","        # 【重要】1ミニバッチ分の「評価（精度検証）」を実行\n","        val_loss,val_acc = valid_step(valid_X, valid_y)\n","\n","        # 取得した損失値と正解率を累計値側に足していく\n","        total_val_loss += val_loss  # 評価用の累計損失値\n","        total_val_acc += val_acc    # 評価用の累計正解数\n","        total_valid += len(valid_y) # 訓練データの累計数\n","\n","    # ミニバッチ単位で累計してきた損失値や正解率の平均を取る\n","    n = epoch + 1                             # 処理済みのエポック数\n","    avg_loss = total_loss / n                 # 訓練用の平均損失値\n","    avg_acc = total_acc / total_train         # 訓練用の平均正解率\n","    avg_val_loss = total_val_loss / n         # 訓練用の平均損失値\n","    avg_val_acc = total_val_acc / total_valid # 訓練用の平均正解率\n","\n","    # グラフ描画のために損失の履歴を保存する\n","    train_history.append(avg_loss)\n","    valid_history.append(avg_val_loss)\n","\n","    # 損失や正解率などの情報を表示\n","    print(f'[Epoch {epoch+1:3d}/{EPOCHS:3d}]' \\\n","          f' loss: {avg_loss:.5f}, acc: {avg_acc:.5f}' \\\n","          f' val_loss: {avg_val_loss:.5f}, val_acc: {avg_val_acc:.5f} At '+datetime.now(timezone('Asia/Tokyo')).strftime('%m/%d %H:%M:%S'))\n","    \n","    # save model\n","    torch.save(model.state_dict(), save_path)\n","print('Finished Training')\n","\n","import matplotlib.pyplot as plt\n","# 学習結果（損失）のグラフを描画\n","epochs = len(train_history)\n","plt.plot(range(epochs), train_history, marker='.', label='loss (Training data)')\n","plt.plot(range(epochs), valid_history, marker='.', label='loss (Validation data)')\n","plt.legend(loc='best')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"],"metadata":{"id":"IxCMTcA5r7M2","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1657385498111,"user_tz":-540,"elapsed":17997476,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"5c2f9fde-bf38-4f34-cbb3-4499512f24fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_len: 910  vaild_len: 228\n","train_iter: 15  vaild_iter: 4\n","Start Training At 07/09 20:51:49\n","[Epoch   1/100] loss: 30.55608, acc: 0.38681 val_loss: 8.97498, val_acc: 0.35965 At 07/09 20:56:00\n","[Epoch   2/100] loss: 14.27632, acc: 0.42088 val_loss: 4.43012, val_acc: 0.35088 At 07/09 21:00:09\n","[Epoch   3/100] loss: 9.19810, acc: 0.45275 val_loss: 2.92636, val_acc: 0.38158 At 07/09 21:04:17\n","[Epoch   4/100] loss: 6.60929, acc: 0.47143 val_loss: 2.18681, val_acc: 0.37281 At 07/09 21:08:26\n","[Epoch   5/100] loss: 5.26508, acc: 0.50769 val_loss: 1.70167, val_acc: 0.39035 At 07/09 21:12:32\n","[Epoch   6/100] loss: 4.26939, acc: 0.53516 val_loss: 1.41586, val_acc: 0.39035 At 07/09 21:16:38\n","[Epoch   7/100] loss: 3.55575, acc: 0.52967 val_loss: 1.20829, val_acc: 0.40789 At 07/09 21:20:46\n","[Epoch   8/100] loss: 3.06512, acc: 0.54835 val_loss: 1.03584, val_acc: 0.39912 At 07/09 21:24:54\n","[Epoch   9/100] loss: 2.60943, acc: 0.55165 val_loss: 0.93343, val_acc: 0.39474 At 07/09 21:29:01\n","[Epoch  10/100] loss: 2.36203, acc: 0.55385 val_loss: 0.81891, val_acc: 0.42105 At 07/09 21:33:07\n","[Epoch  11/100] loss: 2.06523, acc: 0.57582 val_loss: 0.72565, val_acc: 0.42544 At 07/09 21:37:14\n","[Epoch  12/100] loss: 1.90139, acc: 0.59121 val_loss: 0.67088, val_acc: 0.44298 At 07/09 21:41:22\n","[Epoch  13/100] loss: 1.71299, acc: 0.57253 val_loss: 0.61404, val_acc: 0.43421 At 07/09 21:45:29\n","[Epoch  14/100] loss: 1.56484, acc: 0.58022 val_loss: 0.56431, val_acc: 0.46930 At 07/09 21:49:35\n","[Epoch  15/100] loss: 1.44641, acc: 0.59451 val_loss: 0.52303, val_acc: 0.46491 At 07/09 21:53:45\n","[Epoch  16/100] loss: 1.31240, acc: 0.60989 val_loss: 0.49473, val_acc: 0.45614 At 07/09 21:57:52\n","[Epoch  17/100] loss: 1.21495, acc: 0.60769 val_loss: 0.45808, val_acc: 0.46491 At 07/09 22:02:02\n","[Epoch  18/100] loss: 1.13433, acc: 0.62637 val_loss: 0.43508, val_acc: 0.47368 At 07/09 22:06:13\n","[Epoch  19/100] loss: 1.09801, acc: 0.61209 val_loss: 0.40939, val_acc: 0.48246 At 07/09 22:10:21\n","[Epoch  20/100] loss: 0.98427, acc: 0.62198 val_loss: 0.38741, val_acc: 0.48246 At 07/09 22:14:30\n","[Epoch  21/100] loss: 0.92901, acc: 0.61978 val_loss: 0.35986, val_acc: 0.50000 At 07/09 22:18:39\n","[Epoch  22/100] loss: 0.88275, acc: 0.64396 val_loss: 0.34784, val_acc: 0.52193 At 07/09 22:22:48\n","[Epoch  23/100] loss: 0.83708, acc: 0.62637 val_loss: 0.33140, val_acc: 0.51754 At 07/09 22:26:56\n","[Epoch  24/100] loss: 0.78476, acc: 0.63407 val_loss: 0.31550, val_acc: 0.51754 At 07/09 22:31:03\n","[Epoch  25/100] loss: 0.73428, acc: 0.63407 val_loss: 0.30526, val_acc: 0.51316 At 07/09 22:35:11\n","[Epoch  26/100] loss: 0.72455, acc: 0.64725 val_loss: 0.29067, val_acc: 0.52632 At 07/09 22:39:18\n","[Epoch  27/100] loss: 0.66983, acc: 0.64286 val_loss: 0.28392, val_acc: 0.52193 At 07/09 22:43:24\n","[Epoch  28/100] loss: 0.63919, acc: 0.65385 val_loss: 0.26862, val_acc: 0.54825 At 07/09 22:47:31\n","[Epoch  29/100] loss: 0.61182, acc: 0.66703 val_loss: 0.25664, val_acc: 0.55263 At 07/09 22:51:37\n","[Epoch  30/100] loss: 0.57240, acc: 0.65824 val_loss: 0.24656, val_acc: 0.52632 At 07/09 22:55:45\n","[Epoch  31/100] loss: 0.56367, acc: 0.66374 val_loss: 0.23811, val_acc: 0.55702 At 07/09 22:59:52\n","[Epoch  32/100] loss: 0.54633, acc: 0.66813 val_loss: 0.23389, val_acc: 0.56579 At 07/09 23:04:01\n","[Epoch  33/100] loss: 0.51653, acc: 0.67802 val_loss: 0.22390, val_acc: 0.57456 At 07/09 23:08:10\n","[Epoch  34/100] loss: 0.48441, acc: 0.67912 val_loss: 0.21668, val_acc: 0.58333 At 07/09 23:12:21\n","[Epoch  35/100] loss: 0.46901, acc: 0.67692 val_loss: 0.20800, val_acc: 0.56140 At 07/09 23:16:30\n","[Epoch  36/100] loss: 0.46216, acc: 0.69341 val_loss: 0.20857, val_acc: 0.57456 At 07/09 23:20:38\n","[Epoch  37/100] loss: 0.44178, acc: 0.69011 val_loss: 0.19706, val_acc: 0.56140 At 07/09 23:24:45\n","[Epoch  38/100] loss: 0.42821, acc: 0.69341 val_loss: 0.19193, val_acc: 0.57018 At 07/09 23:28:52\n","[Epoch  39/100] loss: 0.40738, acc: 0.69121 val_loss: 0.18516, val_acc: 0.57895 At 07/09 23:32:59\n","[Epoch  40/100] loss: 0.39877, acc: 0.69890 val_loss: 0.17988, val_acc: 0.57895 At 07/09 23:37:04\n","[Epoch  41/100] loss: 0.38051, acc: 0.71538 val_loss: 0.17686, val_acc: 0.57456 At 07/09 23:41:09\n","[Epoch  42/100] loss: 0.35339, acc: 0.71538 val_loss: 0.17115, val_acc: 0.55702 At 07/09 23:45:14\n","[Epoch  43/100] loss: 0.34623, acc: 0.71099 val_loss: 0.16629, val_acc: 0.57456 At 07/09 23:49:17\n","[Epoch  44/100] loss: 0.34650, acc: 0.71538 val_loss: 0.16513, val_acc: 0.58772 At 07/09 23:53:22\n","[Epoch  45/100] loss: 0.33999, acc: 0.71429 val_loss: 0.15782, val_acc: 0.58772 At 07/09 23:57:27\n","[Epoch  46/100] loss: 0.31322, acc: 0.73077 val_loss: 0.15381, val_acc: 0.59211 At 07/10 00:01:31\n","[Epoch  47/100] loss: 0.30725, acc: 0.72308 val_loss: 0.15179, val_acc: 0.59211 At 07/10 00:05:36\n","[Epoch  48/100] loss: 0.29115, acc: 0.74066 val_loss: 0.14744, val_acc: 0.58772 At 07/10 00:09:41\n","[Epoch  49/100] loss: 0.28385, acc: 0.72747 val_loss: 0.14491, val_acc: 0.59211 At 07/10 00:13:44\n","[Epoch  50/100] loss: 0.27915, acc: 0.73297 val_loss: 0.14066, val_acc: 0.60088 At 07/10 00:17:48\n","[Epoch  51/100] loss: 0.26673, acc: 0.74286 val_loss: 0.13810, val_acc: 0.58772 At 07/10 00:21:52\n","[Epoch  52/100] loss: 0.25781, acc: 0.73077 val_loss: 0.13708, val_acc: 0.59211 At 07/10 00:25:55\n","[Epoch  53/100] loss: 0.25046, acc: 0.74945 val_loss: 0.13210, val_acc: 0.57456 At 07/10 00:29:58\n","[Epoch  54/100] loss: 0.25068, acc: 0.74725 val_loss: 0.13194, val_acc: 0.58772 At 07/10 00:34:02\n","[Epoch  55/100] loss: 0.23342, acc: 0.75934 val_loss: 0.12485, val_acc: 0.57895 At 07/10 00:38:05\n","[Epoch  56/100] loss: 0.22794, acc: 0.76374 val_loss: 0.12416, val_acc: 0.58333 At 07/10 00:42:09\n","[Epoch  57/100] loss: 0.22460, acc: 0.75934 val_loss: 0.12237, val_acc: 0.59211 At 07/10 00:46:13\n","[Epoch  58/100] loss: 0.22227, acc: 0.75714 val_loss: 0.11868, val_acc: 0.59211 At 07/10 00:50:17\n","[Epoch  59/100] loss: 0.21656, acc: 0.75714 val_loss: 0.11495, val_acc: 0.59649 At 07/10 00:54:22\n","[Epoch  60/100] loss: 0.20428, acc: 0.76923 val_loss: 0.11398, val_acc: 0.58333 At 07/10 00:58:28\n","[Epoch  61/100] loss: 0.19427, acc: 0.78132 val_loss: 0.11178, val_acc: 0.59649 At 07/10 01:02:32\n","[Epoch  62/100] loss: 0.18877, acc: 0.77692 val_loss: 0.11123, val_acc: 0.58333 At 07/10 01:06:36\n","[Epoch  63/100] loss: 0.18819, acc: 0.77143 val_loss: 0.10925, val_acc: 0.59211 At 07/10 01:10:41\n","[Epoch  64/100] loss: 0.18244, acc: 0.79231 val_loss: 0.10681, val_acc: 0.58772 At 07/10 01:14:44\n","[Epoch  65/100] loss: 0.17432, acc: 0.79670 val_loss: 0.10585, val_acc: 0.59649 At 07/10 01:18:50\n","[Epoch  66/100] loss: 0.17972, acc: 0.80330 val_loss: 0.10574, val_acc: 0.57895 At 07/10 01:22:54\n","[Epoch  67/100] loss: 0.17212, acc: 0.78242 val_loss: 0.10355, val_acc: 0.59649 At 07/10 01:26:57\n","[Epoch  68/100] loss: 0.17337, acc: 0.79011 val_loss: 0.09890, val_acc: 0.58772 At 07/10 01:31:02\n","[Epoch  69/100] loss: 0.15738, acc: 0.80440 val_loss: 0.09842, val_acc: 0.59649 At 07/10 01:35:07\n","[Epoch  70/100] loss: 0.15402, acc: 0.81648 val_loss: 0.09607, val_acc: 0.60088 At 07/10 01:39:13\n","[Epoch  71/100] loss: 0.15477, acc: 0.80879 val_loss: 0.09641, val_acc: 0.59211 At 07/10 01:43:17\n","[Epoch  72/100] loss: 0.15364, acc: 0.81209 val_loss: 0.09366, val_acc: 0.59211 At 07/10 01:47:22\n","[Epoch  73/100] loss: 0.14402, acc: 0.82418 val_loss: 0.09291, val_acc: 0.58772 At 07/10 01:51:27\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-22fd4398f52f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mtotal_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m      \u001b[0;31m# 「評価」時における累計「データ数」\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m# 【重要】1ミニバッチ分の「訓練」を実行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-416009b328c1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \"\"\"\n\u001b[1;32m    229\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \"\"\"\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"EbYyf9S9dzpP"},"source":["# 部材種類分類モデル\n","DenseNet\n","https://pytorch.org/hub/pytorch_vision_densenet/\n","https://atmarkit.itmedia.co.jp/ait/articles/2002/20/news029.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i76vY6NlVJwC"},"outputs":[],"source":["import torch\n","from torch import nn\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n","model.classifier=nn.Linear(1024,df.loc[:,\"encoded_category\"].max()+1,bias=True)\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQYv1-ALVJAf"},"outputs":[],"source":["#https://tzmi.hatenablog.com/entry/2020/03/05/222813\n","load_path = '/content/drive/MyDrive/signate/タナチョー 部材の画像認識/model/category/model20220708_134723.pth'\n","if torch.cuda.is_available():\n","  model.load_state_dict(torch.load(load_path))\n","else:\n","  model.load_state_dict(torch.load(load_path, map_location=torch.device('cpu')))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6409,"status":"ok","timestamp":1657268324828,"user":{"displayName":"ooo ooo","userId":"16849343889828848051"},"user_tz":-540},"id":"O2aWnb_zz_lH","outputId":"7443c9a4-621a-4840-d799-d6c0b90dea6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["train_len: 910  vaild_len: 228\n","train_iter: 15  vaild_iter: 4\n"]}],"source":["import torch.optim as optim   # 「最適化」モジュールの別名定義\n","\n","# 定数（学習方法設計時に必要となるもの）\n","LEARNING_RATE = 1e-3   # 学習率： 0.03\n","REGULARIZATION = 0.03  # 正則化率： 0.03\n","\n","# オプティマイザーを作成（パラメーターと学習率も指定）\n","optimizer = optim.SGD(           # 最適化アルゴリズムに「SGD」を選択\n","    model.parameters(),          # 最適化で更新対象のパラメーター（重みやバイアス）\n","    lr=LEARNING_RATE,            # 更新時の学習率\n","    weight_decay=REGULARIZATION) # L2正則化（※不要な場合は0か省略）\n","\n","# 変数（学習方法設計時に必要となるもの）\n","criterion = nn.CrossEntropyLoss()  # 損失関数\n","\n","images=MyImageFolder(\"/content/drive/MyDrive/signate/タナチョー 部材の画像認識/train\",ImageTransform(),df.loc[:,\"encoded_category\"])\n","\n","batch_size = 64\n","images_size=len(images)\n","train_size = int((images_size * 0.8)//1)\n","dataset_train = Subset(images,list(range(0,train_size)))\n","dataset_valid   = Subset(images, list(range(train_size,images_size)))\n","print(\"train_len:\",len(dataset_train),\" vaild_len:\",len(dataset_valid))\n","\n","loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n","loader_valid = DataLoader(dataset_valid, batch_size=batch_size)\n","print(\"train_iter:\",len(loader_train),\" vaild_iter:\",len(loader_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4I7SUoZqUWaj","executionInfo":{"status":"error","timestamp":1657282572086,"user_tz":-540,"elapsed":14244014,"user":{"displayName":"ooo ooo","userId":"16849343889828848051"}},"outputId":"bc1434c1-4f2b-4e2d-b9a9-2807a405aa3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Start Training At 07/08 17:18:56\n","[Epoch   1/ 50] loss: 1.38518, acc: 0.97473 val_loss: 0.85260, val_acc: 0.91228 At 07/08 17:37:48\n","[Epoch   2/ 50] loss: 0.68304, acc: 0.98242 val_loss: 0.37624, val_acc: 0.92982 At 07/08 17:52:26\n","[Epoch   3/ 50] loss: 0.49089, acc: 0.97253 val_loss: 0.24584, val_acc: 0.91228 At 07/08 18:07:07\n","[Epoch   4/ 50] loss: 0.34204, acc: 0.97363 val_loss: 0.19102, val_acc: 0.92105 At 07/08 18:21:46\n","[Epoch   5/ 50] loss: 0.24355, acc: 0.98132 val_loss: 0.15876, val_acc: 0.91228 At 07/08 18:36:31\n","[Epoch   6/ 50] loss: 0.18875, acc: 0.98132 val_loss: 0.10381, val_acc: 0.92982 At 07/08 18:51:15\n","[Epoch   7/ 50] loss: 0.19748, acc: 0.97253 val_loss: 0.10009, val_acc: 0.92544 At 07/08 19:06:00\n","[Epoch   8/ 50] loss: 0.14051, acc: 0.98132 val_loss: 0.09096, val_acc: 0.90789 At 07/08 19:20:50\n","[Epoch   9/ 50] loss: 0.14388, acc: 0.97802 val_loss: 0.07896, val_acc: 0.91228 At 07/08 19:35:38\n","[Epoch  10/ 50] loss: 0.11568, acc: 0.98132 val_loss: 0.07937, val_acc: 0.92105 At 07/08 19:50:28\n","[Epoch  11/ 50] loss: 0.10592, acc: 0.97692 val_loss: 0.07502, val_acc: 0.90789 At 07/08 20:05:22\n","[Epoch  12/ 50] loss: 0.09039, acc: 0.98791 val_loss: 0.05950, val_acc: 0.92544 At 07/08 20:20:19\n","[Epoch  13/ 50] loss: 0.07980, acc: 0.97912 val_loss: 0.05039, val_acc: 0.91667 At 07/08 20:35:20\n","[Epoch  14/ 50] loss: 0.07982, acc: 0.98462 val_loss: 0.05374, val_acc: 0.89912 At 07/08 20:50:26\n","[Epoch  15/ 50] loss: 0.06709, acc: 0.98462 val_loss: 0.04854, val_acc: 0.92544 At 07/08 21:05:35\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-dd7f5f5d8a48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# 【重要】1ミニバッチ分の「訓練」を実行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# 取得した損失値と正解率を累計値側に足していく\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-ab98525d53ed>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(train_X, train_y)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# フォワードプロパゲーションで出力結果を取得\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#train_X                # 入力データ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 出力結果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#train_y                # 正解ラベル\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, init_features)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minit_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_checkpoint_bottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mbn_function\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbn_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mconcated_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcated_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: T484\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbottleneck_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from datetime import datetime\n","from pytz import timezone\n","save_path='/content/drive/MyDrive/signate/タナチョー 部材の画像認識/model/category/model'+datetime.now(timezone('Asia/Tokyo')).strftime('%Y%m%d_%H%M%S')+'.pth'\n","\n","\n","\n","# 定数（学習／評価時に必要となるもの）\n","EPOCHS = 50        # エポック数： 100\n","\n","# 変数（学習／評価時に必要となるもの）\n","avg_loss = 0.0           # 「訓練」用の平均「損失値」\n","avg_acc = 0.0            # 「訓練」用の平均「正解率」\n","avg_val_loss = 0.0       # 「評価」用の平均「損失値」\n","avg_val_acc = 0.0        # 「評価」用の平均「正解率」\n","\n","# 損失の履歴を保存するための変数\n","train_history = []\n","valid_history = []\n","\n","print(\"Start Training At \"+datetime.now(timezone('Asia/Tokyo')).strftime('%m/%d %H:%M:%S'))\n","\n","for epoch in range(EPOCHS):\n","    # forループ内で使う変数と、エポックごとの値リセット\n","    total_loss = 0.0     # 「訓練」時における累計「損失値」\n","    total_acc = 0.0      # 「訓練」時における累計「正解数」\n","    total_val_loss = 0.0 # 「評価」時における累計「損失値」\n","    total_val_acc = 0.0  # 「評価」時における累計「正解数」\n","    total_train = 0      # 「訓練」時における累計「データ数」\n","    total_valid = 0      # 「評価」時における累計「データ数」\n","\n","    for train_X, train_y in loader_train:\n","        # 【重要】1ミニバッチ分の「訓練」を実行\n","        loss,acc = train_step(train_X, train_y)\n","\n","        # 取得した損失値と正解率を累計値側に足していく\n","        total_loss += loss          # 訓練用の累計損失値\n","        total_acc += acc            # 訓練用の累計正解数\n","        total_train += len(train_y) # 訓練データの累計数\n","            \n","    for valid_X, valid_y in loader_valid:\n","        # 【重要】1ミニバッチ分の「評価（精度検証）」を実行\n","        val_loss,val_acc = valid_step(valid_X, valid_y)\n","\n","        # 取得した損失値と正解率を累計値側に足していく\n","        total_val_loss += val_loss  # 評価用の累計損失値\n","        total_val_acc += val_acc    # 評価用の累計正解数\n","        total_valid += len(valid_y) # 訓練データの累計数\n","\n","    # ミニバッチ単位で累計してきた損失値や正解率の平均を取る\n","    n = epoch + 1                             # 処理済みのエポック数\n","    avg_loss = total_loss / n                 # 訓練用の平均損失値\n","    avg_acc = total_acc / total_train         # 訓練用の平均正解率\n","    avg_val_loss = total_val_loss / n         # 訓練用の平均損失値\n","    avg_val_acc = total_val_acc / total_valid # 訓練用の平均正解率\n","\n","    # グラフ描画のために損失の履歴を保存する\n","    train_history.append(avg_loss)\n","    valid_history.append(avg_val_loss)\n","\n","    # 損失や正解率などの情報を表示\n","    print(f'[Epoch {epoch+1:3d}/{EPOCHS:3d}]' \\\n","          f' loss: {avg_loss:.5f}, acc: {avg_acc:.5f}' \\\n","          f' val_loss: {avg_val_loss:.5f}, val_acc: {avg_val_acc:.5f} At '+datetime.now(timezone('Asia/Tokyo')).strftime('%m/%d %H:%M:%S'))\n","    \n","    # save model\n","    torch.save(model.state_dict(), save_path)\n","print('Finished Training')\n","\n","import matplotlib.pyplot as plt\n","# 学習結果（損失）のグラフを描画\n","epochs = len(train_history)\n","plt.plot(range(epochs), train_history, marker='.', label='loss (Training data)')\n","plt.plot(range(epochs), valid_history, marker='.', label='loss (Validation data)')\n","plt.legend(loc='best')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-sGw50u4fvza"},"source":["# 部材色分類モデル"]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n","model.classifier=nn.Linear(1024, df.loc[:,\"encoded_color\"].max()+1,bias=True)\n","model.to(device)"],"metadata":{"id":"M4s0yG6Zy22y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#https://tzmi.hatenablog.com/entry/2020/03/05/222813\n","load_path = '/content/drive/MyDrive/signate/タナチョー 部材の画像認識/model/color/model20220708_134723.pth'\n","if torch.cuda.is_available():\n","  model.load_state_dict(torch.load(load_path))\n","else:\n","  model.load_state_dict(torch.load(load_path, map_location=torch.device('cpu')))"],"metadata":{"id":"XmsgtrHLy5hR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmuGaPcvgAKM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"de947281-0c10-4d93-e8c6-6f6e19f22369"},"outputs":[{"output_type":"stream","name":"stdout","text":["train_len: 910  vaild_len: 228\n","train_iter: 15  vaild_iter: 4\n","Start Training At 07/08 21:41:13\n","[Epoch   1/200] loss: 27.12253, acc: 0.48132 val_loss: 7.40855, val_acc: 0.43860 At 07/08 21:56:47\n","[Epoch   2/200] loss: 10.60821, acc: 0.61758 val_loss: 3.72420, val_acc: 0.43860 At 07/08 22:00:50\n","[Epoch   3/200] loss: 6.79712, acc: 0.61758 val_loss: 2.43883, val_acc: 0.44298 At 07/08 22:04:50\n","[Epoch   4/200] loss: 4.91297, acc: 0.61758 val_loss: 1.82631, val_acc: 0.44298 At 07/08 22:08:51\n","[Epoch   5/200] loss: 3.86994, acc: 0.61868 val_loss: 1.44759, val_acc: 0.43860 At 07/08 22:12:50\n","[Epoch   6/200] loss: 3.06532, acc: 0.61868 val_loss: 1.18883, val_acc: 0.44298 At 07/08 22:16:51\n","[Epoch   7/200] loss: 2.64666, acc: 0.62198 val_loss: 1.02826, val_acc: 0.43860 At 07/08 22:20:51\n","[Epoch   8/200] loss: 2.28720, acc: 0.62418 val_loss: 0.90240, val_acc: 0.44298 At 07/08 22:24:52\n","[Epoch   9/200] loss: 1.98481, acc: 0.62527 val_loss: 0.77068, val_acc: 0.45614 At 07/08 22:28:51\n","[Epoch  10/200] loss: 1.77517, acc: 0.62857 val_loss: 0.69419, val_acc: 0.46930 At 07/08 22:32:52\n","[Epoch  11/200] loss: 1.61776, acc: 0.63407 val_loss: 0.62967, val_acc: 0.48246 At 07/08 22:36:52\n","[Epoch  12/200] loss: 1.46742, acc: 0.65604 val_loss: 0.57395, val_acc: 0.48246 At 07/08 22:40:50\n","[Epoch  13/200] loss: 1.26974, acc: 0.66593 val_loss: 0.52028, val_acc: 0.48684 At 07/08 22:44:51\n","[Epoch  14/200] loss: 1.21552, acc: 0.64835 val_loss: 0.48560, val_acc: 0.47368 At 07/08 22:48:51\n","[Epoch  15/200] loss: 1.08119, acc: 0.67582 val_loss: 0.44350, val_acc: 0.53509 At 07/08 22:52:53\n","[Epoch  16/200] loss: 1.02220, acc: 0.66374 val_loss: 0.41713, val_acc: 0.53070 At 07/08 22:56:54\n","[Epoch  17/200] loss: 0.93511, acc: 0.67253 val_loss: 0.38683, val_acc: 0.53509 At 07/08 23:00:55\n","[Epoch  18/200] loss: 0.90351, acc: 0.69560 val_loss: 0.36313, val_acc: 0.53509 At 07/08 23:04:55\n","[Epoch  19/200] loss: 0.81582, acc: 0.69231 val_loss: 0.33926, val_acc: 0.57456 At 07/08 23:08:54\n","[Epoch  20/200] loss: 0.75922, acc: 0.70659 val_loss: 0.32393, val_acc: 0.55702 At 07/08 23:12:54\n","[Epoch  21/200] loss: 0.72169, acc: 0.70769 val_loss: 0.30642, val_acc: 0.56579 At 07/08 23:16:53\n","[Epoch  22/200] loss: 0.68959, acc: 0.71429 val_loss: 0.28900, val_acc: 0.58772 At 07/08 23:20:53\n","[Epoch  23/200] loss: 0.64256, acc: 0.71209 val_loss: 0.28363, val_acc: 0.56579 At 07/08 23:24:52\n","[Epoch  24/200] loss: 0.60740, acc: 0.72967 val_loss: 0.26371, val_acc: 0.55263 At 07/08 23:28:52\n","[Epoch  25/200] loss: 0.56752, acc: 0.73077 val_loss: 0.25412, val_acc: 0.57456 At 07/08 23:32:50\n","[Epoch  26/200] loss: 0.53824, acc: 0.73736 val_loss: 0.24579, val_acc: 0.57895 At 07/08 23:36:49\n","[Epoch  27/200] loss: 0.53617, acc: 0.73407 val_loss: 0.22678, val_acc: 0.57018 At 07/08 23:40:49\n","[Epoch  28/200] loss: 0.49089, acc: 0.73626 val_loss: 0.22392, val_acc: 0.60088 At 07/08 23:44:49\n","[Epoch  29/200] loss: 0.47758, acc: 0.74066 val_loss: 0.21609, val_acc: 0.58772 At 07/08 23:48:49\n","[Epoch  30/200] loss: 0.45156, acc: 0.74176 val_loss: 0.20725, val_acc: 0.59649 At 07/08 23:52:50\n","[Epoch  31/200] loss: 0.41817, acc: 0.73956 val_loss: 0.19839, val_acc: 0.60088 At 07/08 23:56:49\n","[Epoch  32/200] loss: 0.41069, acc: 0.73516 val_loss: 0.18995, val_acc: 0.58772 At 07/09 00:00:49\n","[Epoch  33/200] loss: 0.41663, acc: 0.74176 val_loss: 0.17991, val_acc: 0.60088 At 07/09 00:04:48\n","[Epoch  34/200] loss: 0.37558, acc: 0.74835 val_loss: 0.17555, val_acc: 0.59649 At 07/09 00:08:47\n","[Epoch  35/200] loss: 0.35510, acc: 0.74286 val_loss: 0.17173, val_acc: 0.60088 At 07/09 00:12:49\n","[Epoch  36/200] loss: 0.34047, acc: 0.74615 val_loss: 0.16908, val_acc: 0.59649 At 07/09 00:16:53\n","[Epoch  37/200] loss: 0.32939, acc: 0.75165 val_loss: 0.16090, val_acc: 0.60088 At 07/09 00:20:54\n","[Epoch  38/200] loss: 0.32835, acc: 0.75385 val_loss: 0.15282, val_acc: 0.59649 At 07/09 00:24:56\n","[Epoch  39/200] loss: 0.31063, acc: 0.75604 val_loss: 0.15376, val_acc: 0.59649 At 07/09 00:28:56\n","[Epoch  40/200] loss: 0.29258, acc: 0.75714 val_loss: 0.14640, val_acc: 0.59649 At 07/09 00:32:55\n","[Epoch  41/200] loss: 0.29183, acc: 0.76703 val_loss: 0.14344, val_acc: 0.59649 At 07/09 00:36:55\n"]}],"source":["import torch.optim as optim   # 「最適化」モジュールの別名定義\n","\n","# 定数（学習方法設計時に必要となるもの）\n","LEARNING_RATE = 1e-3   # 学習率： 0.03\n","REGULARIZATION = 0.03  # 正則化率： 0.03\n","\n","# オプティマイザーを作成（パラメーターと学習率も指定）\n","optimizer = optim.SGD(           # 最適化アルゴリズムに「SGD」を選択\n","    model.parameters(),          # 最適化で更新対象のパラメーター（重みやバイアス）\n","    lr=LEARNING_RATE,            # 更新時の学習率\n","    weight_decay=REGULARIZATION) # L2正則化（※不要な場合は0か省略）\n","\n","# 変数（学習方法設計時に必要となるもの）\n","criterion = nn.CrossEntropyLoss()  # 損失関数\n","\n","\n","\n","images=MyImageFolder(\"/content/drive/MyDrive/signate/タナチョー 部材の画像認識/train\",ImageTransform(),df.loc[:,\"encoded_color\"])\n","\n","batch_size = 64\n","images_size=len(images)\n","train_size = int((images_size * 0.8)//1)\n","dataset_train = Subset(images,list(range(0,train_size)))\n","dataset_valid   = Subset(images, list(range(train_size,images_size)))\n","print(\"train_len:\",len(dataset_train),\" vaild_len:\",len(dataset_valid))\n","\n","loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n","loader_valid = DataLoader(dataset_valid, batch_size=batch_size)\n","print(\"train_iter:\",len(loader_train),\" vaild_iter:\",len(loader_valid))\n","\n","\n","from datetime import datetime\n","from pytz import timezone\n","save_path='/content/drive/MyDrive/signate/タナチョー 部材の画像認識/model/color/model'+datetime.now(timezone('Asia/Tokyo')).strftime('%Y%m%d_%H%M%S')+'.pth'\n","\n","\n","\n","# 定数（学習／評価時に必要となるもの）\n","EPOCHS = 200         # エポック数： 100\n","\n","# 変数（学習／評価時に必要となるもの）\n","avg_loss = 0.0           # 「訓練」用の平均「損失値」\n","avg_acc = 0.0            # 「訓練」用の平均「正解率」\n","avg_val_loss = 0.0       # 「評価」用の平均「損失値」\n","avg_val_acc = 0.0        # 「評価」用の平均「正解率」\n","\n","# 損失の履歴を保存するための変数\n","train_history = []\n","valid_history = []\n","\n","print(\"Start Training At \"+datetime.now(timezone('Asia/Tokyo')).strftime('%m/%d %H:%M:%S'))\n","\n","for epoch in range(EPOCHS):\n","    # forループ内で使う変数と、エポックごとの値リセット\n","    total_loss = 0.0     # 「訓練」時における累計「損失値」\n","    total_acc = 0.0      # 「訓練」時における累計「正解数」\n","    total_val_loss = 0.0 # 「評価」時における累計「損失値」\n","    total_val_acc = 0.0  # 「評価」時における累計「正解数」\n","    total_train = 0      # 「訓練」時における累計「データ数」\n","    total_valid = 0      # 「評価」時における累計「データ数」\n","\n","    for train_X, train_y in loader_train:\n","        # 【重要】1ミニバッチ分の「訓練」を実行\n","        loss,acc = train_step(train_X, train_y)\n","\n","        # 取得した損失値と正解率を累計値側に足していく\n","        total_loss += loss          # 訓練用の累計損失値\n","        total_acc += acc            # 訓練用の累計正解数\n","        total_train += len(train_y) # 訓練データの累計数\n","            \n","    for valid_X, valid_y in loader_valid:\n","        # 【重要】1ミニバッチ分の「評価（精度検証）」を実行\n","        val_loss,val_acc = valid_step(valid_X, valid_y)\n","\n","        # 取得した損失値と正解率を累計値側に足していく\n","        total_val_loss += val_loss  # 評価用の累計損失値\n","        total_val_acc += val_acc    # 評価用の累計正解数\n","        total_valid += len(valid_y) # 訓練データの累計数\n","\n","    # ミニバッチ単位で累計してきた損失値や正解率の平均を取る\n","    n = epoch + 1                             # 処理済みのエポック数\n","    avg_loss = total_loss / n                 # 訓練用の平均損失値\n","    avg_acc = total_acc / total_train         # 訓練用の平均正解率\n","    avg_val_loss = total_val_loss / n         # 訓練用の平均損失値\n","    avg_val_acc = total_val_acc / total_valid # 訓練用の平均正解率\n","\n","    # グラフ描画のために損失の履歴を保存する\n","    train_history.append(avg_loss)\n","    valid_history.append(avg_val_loss)\n","\n","    # 損失や正解率などの情報を表示\n","    print(f'[Epoch {epoch+1:3d}/{EPOCHS:3d}]' \\\n","          f' loss: {avg_loss:.5f}, acc: {avg_acc:.5f}' \\\n","          f' val_loss: {avg_val_loss:.5f}, val_acc: {avg_val_acc:.5f} At '+datetime.now(timezone('Asia/Tokyo')).strftime('%m/%d %H:%M:%S'))\n","    \n","    # save model\n","    torch.save(model.state_dict(), save_path)\n","print('Finished Training')\n","\n","import matplotlib.pyplot as plt\n","# 学習結果（損失）のグラフを描画\n","epochs = len(train_history)\n","plt.plot(range(epochs), train_history, marker='.', label='loss (Training data)')\n","plt.plot(range(epochs), valid_history, marker='.', label='loss (Validation data)')\n","plt.legend(loc='best')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]},{"cell_type":"code","source":["\n","for valid_X, valid_y in loader_valid:\n","        # 【重要】1ミニバッチ分の「評価（精度検証）」を実行\n","        val_loss,val_acc = valid_step(valid_X, valid_y)\n","\n","        # 取得した損失値と正解率を累計値側に足していく\n","        total_val_loss += val_loss  # 評価用の累計損失値\n","        total_val_acc += val_acc    # 評価用の累計正解数\n","        total_valid += len(valid_y) # 訓練データの累計数\n","\n","def valid_step(valid_X, valid_y):\n","    valid_X, valid_y=valid_X.to(device),valid_y.to(device)\n","\n","    # 評価モードに設定（※dropoutなどの挙動が評価用になる）\n","    model.eval()\n","    \n","    # フォワードプロパゲーションで出力結果を取得\n","    #valid_X                # 入力データ\n","    pred_y = model(valid_X) # 出力結果\n","    #valid_y                # 正解ラベル\n","\n","    # 出力結果と正解ラベルから損失を計算\n","    loss = criterion(pred_y, valid_y)     # 誤差（出力結果と正解ラベルの差）から損失を取得\n","\n","    # 正解率を算出\n","    with torch.no_grad(): # 勾配は計算しないモードにする\n","        acc=0\n","        for i in range(len(valid_y)):\n","          acc+=1 if torch.argmax(pred_y[i])==valid_y[i] else 0\n","\n","    # 損失と正解数をタプルで返す\n","    return (loss.item(), acc)  # ※item()=Pythonの数値"],"metadata":{"id":"4LrRQobiAVTN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kzq5LpUigD-J"},"source":["# 部材型番分類モデル"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12559,"status":"ok","timestamp":1657191603094,"user":{"displayName":"ooo ooo","userId":"16849343889828848051"},"user_tz":-540},"id":"Y7EWgQgZijNl","outputId":"212324fb-5533-42fc-d146-6c8b7a40b1c3"},"outputs":[{"data":{"text/plain":["(<PIL.Image.Image image mode=RGB size=224x224 at 0x7F67683CAC10>, 0)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","class TestTransform():\n","  def __init__(self):\n","    self.data_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","    ])\n","  def __call__(self, img):\n","    return self.data_transform(img)\n","test_images=datasets.ImageFolder( \"/content/drive/MyDrive/signate/タナチョー 部材の画像認識/train\", transform = TestTransform())\n","test_images[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RS903KnqRULR"},"outputs":[],"source":["\n","!kill -9 $(lsof -t)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Main.ipynb","toc_visible":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}