{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Zl-q2-GaVEKY","9mV3AKIa6scu","sMGzwZbhCNkE"],"mount_file_id":"1reECNR2p5iiWgfSV-rnU9kujMOTg1B6C","authorship_tag":"ABX9TyN5z8A5KxNvs9OaowXgRtla"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Qa_kXD2cR52z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676552231869,"user_tz":-540,"elapsed":18148,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"05757e60-2987-450a-97e1-6045e53ab4cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","root=\"/content/drive/MyDrive/signate/ワールドＡＩ/\"\n","anomaly_root=root+\"anomaly/\"\n","\n","import cv2\n","import os\n","import json\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import glob\n","import json\n","import shutil\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","import os\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","train_path=root+\"train/\"\n","h=1080\n","w=1920\n","\n","category={\"要補修-1.区画線\":0,\n"," \"要補修-2.道路標識\":1,\n"," \"要補修-3.照明\":2,\n"," \"補修不要-1.区画線\":0,\n"," \"補修不要-2.道路標識\":1,\n"," \"補修不要-3.照明\":2}\n","\n","anomaly_data_path=anomaly_root+\"anomaly_data/\""]},{"cell_type":"markdown","source":["# データセット"],"metadata":{"id":"Zl-q2-GaVEKY"}},{"cell_type":"code","source":["video_files = glob.glob(train_path+\"*.mp4\")\n","base_names=[]\n","for file_path in video_files:\n","    base_name=os.path.basename(file_path)[:-4]\n","    base_names.append(base_name)"],"metadata":{"id":"WzIZbvmNvO9q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["category_names=[\"要補修-1.区画線\",\n"," \"要補修-2.道路標識\",\n"," \"要補修-3.照明\",\n"," \"補修不要-1.区画線\",\n"," \"補修不要-2.道路標識\",\n"," \"補修不要-3.照明\"]\n","for n in category_names:\n","    os.makedirs(anomaly_data_path+n, exist_ok=True)"],"metadata":{"id":"UmNWiWRe1owF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for bn in base_names:\n","    with open(f'{train_path}{bn}.json') as f:\n","        data_list = json.load(f)\n","    cap = cv2.VideoCapture(f\"{train_path}{bn}.mp4\")\n","    for data in data_list:\n","        frame_id=data[\"frame_id\"]\n","        labels=data[\"labels\"]\n","        ret, frame = cap.read()\n","        im=Image.fromarray(frame)\n","        for k,boxes in labels.items():\n","            for i,box in enumerate(boxes):\n","                im.crop((box[0][0],box[0][1],box[1][0],box[1][1])).save(f'{anomaly_data_path}{k}/crop_{bn}_{frame_id}_{i}.jpg', quality=95)"],"metadata":{"id":"Uts2JzvKv6AV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["glob.glob(train_path+\"*.mp4\")"],"metadata":{"id":"8u7JeLJd3SnK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_paths=glob.glob(anomaly_data_path+\"*/*/*.jpg\")\n","for p in img_paths:\n","    img = Image.open(p)\n","    img_resize = img.resize((224, 224))\n","    img_resize.save(p)"],"metadata":{"id":"Nsc9u5i2cSjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i,n in enumerate([\"区画線\",\"道路標識\",\"照明\"]):\n","    for ano in [\"要補修-\",\"補修不要-\"]:\n","        file_num=len(glob.glob((f\"{anomaly_data_path}{n}/{ano}{i+1}.{n}/*\")))\n","        print(f\"{ano}{i+1}.{n}:{file_num}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhX09X0aCcJt","executionInfo":{"status":"ok","timestamp":1676431692061,"user_tz":-540,"elapsed":147754,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"92d12184-411f-47c2-c189-8cce4f9cda8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["要補修-1.区画線:17771\n","補修不要-1.区画線:7101\n","要補修-2.道路標識:223\n","補修不要-2.道路標識:4521\n","要補修-3.照明:242\n","補修不要-3.照明:5871\n"]}]},{"cell_type":"markdown","source":["# 学習"],"metadata":{"id":"6hRolE1fVHkV"}},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UiBXHHocFiff","executionInfo":{"status":"ok","timestamp":1676552238214,"user_tz":-540,"elapsed":6352,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"703b2ce1-940f-405e-9b0d-656aa2f86a05"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.7.2\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torchinfo import summary\n","import matplotlib.pyplot as plt\n","import time\n","import os \n","import copy\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","#cuda:0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CIzfYNrb8rjZ","executionInfo":{"status":"ok","timestamp":1676552241870,"user_tz":-540,"elapsed":3677,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"934c6fbe-ebbf-4877-8fff-1353eb8f0be7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"code","source":["def tl(category):\n","#category=\"道路標識\"\n","    data_transform=transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","\n","    data_dir = anomaly_data_path+category\n","    image_dataset = datasets.ImageFolder(data_dir,data_transform)\n","                    \n","    train_dataset, val_dataset = torch.utils.data.random_split(\n","        image_dataset, lengths=[0.9,0.1], generator=torch.Generator().manual_seed(42)\n","    )\n","    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=600,\n","                                                shuffle=True, num_workers=2)\n","    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=600,\n","                                                shuffle=True, num_workers=2)\n","    dataloaders={\"train\":train_dataloader,\"val\":val_dataloader}\n","\n","    dataset_sizes = {\"train\": len(train_dataset), 'val':len(val_dataset)}\n","    class_names = image_dataset.classes\n","    class_num=len(class_names)\n","    print(class_names)\n","\n","    model = models.efficientnet_v2_s(pretrained=True)\n","    model.classifier = nn.Sequential(\n","        nn.Dropout(p=0.2, inplace=True),\n","        nn.Linear(in_features=1280, out_features=class_num, bias=True)\n","    )\n","\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    last_layers = list(model.children())[-2:]\n","    for last_layer in last_layers:\n","        for param in last_layer.parameters():\n","            param.requires_grad = True\n","\n","    #print(model)\n","    model=torch.load(os.path.join(anomaly_root,f\"{category}_7.pth\"))\n","    print(summary(model=model))\n","\n","\n","\n","    model = model.to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Observe that all parameters are being optimized\n","    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","    # Decay LR by a factor of 0.1 every 7 epochs\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","    since = time.time()\n","\n","\n","    num_epochs=100\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                scheduler.step()\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    #tensor(max, max_indices)なのでpredは0,1のラベル\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        torch.save(model,f\"{anomaly_root}{category}_{8+epoch}.pth\")\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    torch.save(model,f\"{anomaly_root}{category}_best.pth\")"],"metadata":{"id":"zCm2BgB-8zU-","executionInfo":{"status":"ok","timestamp":1676554678362,"user_tz":-540,"elapsed":452,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["for c in [\"照明\"]:\n","    tl(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xin8DBlZ_7gy","outputId":"8b30a14b-1d27-4390-9201-9ecc7a65b30a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['その他', '補修不要', '要補修']\n","================================================================================\n","Layer (type:depth-idx)                                  Param #\n","================================================================================\n","EfficientNet                                            --\n","├─Sequential: 1-1                                       --\n","│    └─Conv2dNormActivation: 2-1                        --\n","│    │    └─Conv2d: 3-1                                 (648)\n","│    │    └─BatchNorm2d: 3-2                            (48)\n","│    │    └─SiLU: 3-3                                   --\n","│    └─Sequential: 2-2                                  --\n","│    │    └─FusedMBConv: 3-4                            (5,232)\n","│    │    └─FusedMBConv: 3-5                            (5,232)\n","│    └─Sequential: 2-3                                  --\n","│    │    └─FusedMBConv: 3-6                            (25,632)\n","│    │    └─FusedMBConv: 3-7                            (92,640)\n","│    │    └─FusedMBConv: 3-8                            (92,640)\n","│    │    └─FusedMBConv: 3-9                            (92,640)\n","│    └─Sequential: 2-4                                  --\n","│    │    └─FusedMBConv: 3-10                           (95,744)\n","│    │    └─FusedMBConv: 3-11                           (164,480)\n","│    │    └─FusedMBConv: 3-12                           (164,480)\n","│    │    └─FusedMBConv: 3-13                           (164,480)\n","│    └─Sequential: 2-5                                  --\n","│    │    └─MBConv: 3-14                                (61,200)\n","│    │    └─MBConv: 3-15                                (171,296)\n","│    │    └─MBConv: 3-16                                (171,296)\n","│    │    └─MBConv: 3-17                                (171,296)\n","│    │    └─MBConv: 3-18                                (171,296)\n","│    │    └─MBConv: 3-19                                (171,296)\n","│    └─Sequential: 2-6                                  --\n","│    │    └─MBConv: 3-20                                (281,440)\n","│    │    └─MBConv: 3-21                                (397,800)\n","│    │    └─MBConv: 3-22                                (397,800)\n","│    │    └─MBConv: 3-23                                (397,800)\n","│    │    └─MBConv: 3-24                                (397,800)\n","│    │    └─MBConv: 3-25                                (397,800)\n","│    │    └─MBConv: 3-26                                (397,800)\n","│    │    └─MBConv: 3-27                                (397,800)\n","│    │    └─MBConv: 3-28                                (397,800)\n","│    └─Sequential: 2-7                                  --\n","│    │    └─MBConv: 3-29                                (490,152)\n","│    │    └─MBConv: 3-30                                (1,005,120)\n","│    │    └─MBConv: 3-31                                (1,005,120)\n","│    │    └─MBConv: 3-32                                (1,005,120)\n","│    │    └─MBConv: 3-33                                (1,005,120)\n","│    │    └─MBConv: 3-34                                (1,005,120)\n","│    │    └─MBConv: 3-35                                (1,005,120)\n","│    │    └─MBConv: 3-36                                (1,005,120)\n","│    │    └─MBConv: 3-37                                (1,005,120)\n","│    │    └─MBConv: 3-38                                (1,005,120)\n","│    │    └─MBConv: 3-39                                (1,005,120)\n","│    │    └─MBConv: 3-40                                (1,005,120)\n","│    │    └─MBConv: 3-41                                (1,005,120)\n","│    │    └─MBConv: 3-42                                (1,005,120)\n","│    │    └─MBConv: 3-43                                (1,005,120)\n","│    └─Conv2dNormActivation: 2-8                        --\n","│    │    └─Conv2d: 3-44                                (327,680)\n","│    │    └─BatchNorm2d: 3-45                           (2,560)\n","│    │    └─SiLU: 3-46                                  --\n","├─AdaptiveAvgPool2d: 1-2                                --\n","├─Sequential: 1-3                                       --\n","│    └─Dropout: 2-9                                     --\n","│    └─Linear: 2-10                                     3,843\n","================================================================================\n","Total params: 20,181,331\n","Trainable params: 3,843\n","Non-trainable params: 20,177,488\n","================================================================================\n","Epoch 0/99\n","----------\n","train Loss: 0.3504 Acc: 0.9179\n","val Loss: 0.3460 Acc: 0.9258\n","\n","Epoch 1/99\n","----------\n","train Loss: 0.3526 Acc: 0.9206\n","val Loss: 0.3425 Acc: 0.9258\n","\n","Epoch 2/99\n","----------\n","train Loss: 0.3445 Acc: 0.9213\n","val Loss: 0.3370 Acc: 0.9273\n","\n","Epoch 3/99\n","----------\n","train Loss: 0.3396 Acc: 0.9216\n","val Loss: 0.3314 Acc: 0.9289\n","\n","Epoch 4/99\n","----------\n","train Loss: 0.3411 Acc: 0.9195\n","val Loss: 0.3271 Acc: 0.9289\n","\n","Epoch 5/99\n","----------\n","train Loss: 0.3384 Acc: 0.9204\n","val Loss: 0.3236 Acc: 0.9289\n","\n","Epoch 6/99\n","----------\n","train Loss: 0.3260 Acc: 0.9227\n","val Loss: 0.3231 Acc: 0.9289\n","\n","Epoch 7/99\n","----------\n","train Loss: 0.3351 Acc: 0.9230\n","val Loss: 0.3235 Acc: 0.9289\n","\n","Epoch 8/99\n","----------\n","train Loss: 0.3321 Acc: 0.9223\n","val Loss: 0.3217 Acc: 0.9289\n","\n","Epoch 9/99\n","----------\n","train Loss: 0.3276 Acc: 0.9230\n","val Loss: 0.3217 Acc: 0.9289\n","\n","Epoch 10/99\n","----------\n","train Loss: 0.3364 Acc: 0.9199\n","val Loss: 0.3222 Acc: 0.9289\n","\n","Epoch 11/99\n","----------\n","train Loss: 0.3288 Acc: 0.9216\n","val Loss: 0.3233 Acc: 0.9289\n","\n","Epoch 12/99\n","----------\n","train Loss: 0.3286 Acc: 0.9227\n","val Loss: 0.3253 Acc: 0.9289\n","\n","Epoch 13/99\n","----------\n","train Loss: 0.3342 Acc: 0.9209\n","val Loss: 0.3248 Acc: 0.9289\n","\n","Epoch 14/99\n","----------\n","train Loss: 0.3310 Acc: 0.9234\n","val Loss: 0.3231 Acc: 0.9289\n","\n","Epoch 15/99\n","----------\n","train Loss: 0.3369 Acc: 0.9206\n","val Loss: 0.3234 Acc: 0.9289\n","\n","Epoch 16/99\n","----------\n","train Loss: 0.3353 Acc: 0.9216\n","val Loss: 0.3218 Acc: 0.9289\n","\n","Epoch 17/99\n","----------\n","train Loss: 0.3304 Acc: 0.9228\n","val Loss: 0.3220 Acc: 0.9305\n","\n","Epoch 18/99\n","----------\n","train Loss: 0.3248 Acc: 0.9237\n","val Loss: 0.3220 Acc: 0.9289\n","\n","Epoch 19/99\n","----------\n","train Loss: 0.3325 Acc: 0.9220\n","val Loss: 0.3222 Acc: 0.9289\n","\n","Epoch 20/99\n","----------\n","train Loss: 0.3274 Acc: 0.9218\n","val Loss: 0.3216 Acc: 0.9305\n","\n","Epoch 21/99\n","----------\n","train Loss: 0.3395 Acc: 0.9213\n","val Loss: 0.3207 Acc: 0.9289\n","\n","Epoch 22/99\n","----------\n","train Loss: 0.3228 Acc: 0.9221\n","val Loss: 0.3210 Acc: 0.9289\n","\n","Epoch 23/99\n","----------\n","train Loss: 0.3322 Acc: 0.9214\n","val Loss: 0.3204 Acc: 0.9289\n","\n","Epoch 24/99\n","----------\n","train Loss: 0.3208 Acc: 0.9227\n","val Loss: 0.3209 Acc: 0.9289\n","\n","Epoch 25/99\n","----------\n","train Loss: 0.3341 Acc: 0.9214\n","val Loss: 0.3199 Acc: 0.9289\n","\n","Epoch 26/99\n","----------\n","train Loss: 0.3266 Acc: 0.9241\n","val Loss: 0.3196 Acc: 0.9289\n","\n","Epoch 27/99\n","----------\n","train Loss: 0.3316 Acc: 0.9223\n","val Loss: 0.3218 Acc: 0.9289\n","\n","Epoch 28/99\n","----------\n","train Loss: 0.3307 Acc: 0.9237\n","val Loss: 0.3223 Acc: 0.9289\n","\n","Epoch 29/99\n","----------\n","train Loss: 0.3368 Acc: 0.9213\n","val Loss: 0.3213 Acc: 0.9289\n","\n","Epoch 30/99\n","----------\n","train Loss: 0.3310 Acc: 0.9200\n","val Loss: 0.3213 Acc: 0.9289\n","\n","Epoch 31/99\n","----------\n","train Loss: 0.3299 Acc: 0.9213\n","val Loss: 0.3204 Acc: 0.9289\n","\n","Epoch 32/99\n","----------\n","train Loss: 0.3243 Acc: 0.9202\n","val Loss: 0.3221 Acc: 0.9289\n","\n","Epoch 33/99\n","----------\n","train Loss: 0.3325 Acc: 0.9218\n","val Loss: 0.3216 Acc: 0.9289\n","\n","Epoch 34/99\n","----------\n","train Loss: 0.3354 Acc: 0.9218\n","val Loss: 0.3214 Acc: 0.9289\n","\n","Epoch 35/99\n","----------\n","train Loss: 0.3265 Acc: 0.9200\n","val Loss: 0.3215 Acc: 0.9289\n","\n","Epoch 36/99\n","----------\n","train Loss: 0.3231 Acc: 0.9232\n","val Loss: 0.3195 Acc: 0.9289\n","\n","Epoch 37/99\n","----------\n","train Loss: 0.3333 Acc: 0.9202\n","val Loss: 0.3205 Acc: 0.9289\n","\n","Epoch 38/99\n","----------\n","train Loss: 0.3281 Acc: 0.9230\n","val Loss: 0.3228 Acc: 0.9289\n","\n","Epoch 39/99\n","----------\n","train Loss: 0.3232 Acc: 0.9213\n","val Loss: 0.3224 Acc: 0.9289\n","\n","Epoch 40/99\n","----------\n","train Loss: 0.3345 Acc: 0.9195\n","val Loss: 0.3238 Acc: 0.9289\n","\n","Epoch 41/99\n","----------\n","train Loss: 0.3294 Acc: 0.9225\n","val Loss: 0.3220 Acc: 0.9289\n","\n","Epoch 42/99\n","----------\n","train Loss: 0.3315 Acc: 0.9211\n","val Loss: 0.3214 Acc: 0.9289\n","\n","Epoch 43/99\n","----------\n","train Loss: 0.3357 Acc: 0.9211\n","val Loss: 0.3214 Acc: 0.9289\n","\n","Epoch 44/99\n","----------\n","train Loss: 0.3243 Acc: 0.9207\n","val Loss: 0.3209 Acc: 0.9289\n","\n","Epoch 45/99\n","----------\n","train Loss: 0.3328 Acc: 0.9232\n","val Loss: 0.3211 Acc: 0.9289\n","\n","Epoch 46/99\n","----------\n","train Loss: 0.3290 Acc: 0.9211\n","val Loss: 0.3206 Acc: 0.9289\n","\n","Epoch 47/99\n","----------\n","train Loss: 0.3204 Acc: 0.9202\n","val Loss: 0.3217 Acc: 0.9289\n","\n","Epoch 48/99\n","----------\n","train Loss: 0.3246 Acc: 0.9221\n","val Loss: 0.3223 Acc: 0.9289\n","\n","Epoch 49/99\n","----------\n","train Loss: 0.3280 Acc: 0.9234\n","val Loss: 0.3214 Acc: 0.9289\n","\n","Epoch 50/99\n","----------\n","train Loss: 0.3352 Acc: 0.9216\n","val Loss: 0.3204 Acc: 0.9289\n","\n","Epoch 51/99\n","----------\n","train Loss: 0.3312 Acc: 0.9207\n","val Loss: 0.3208 Acc: 0.9289\n","\n","Epoch 52/99\n","----------\n","train Loss: 0.3329 Acc: 0.9213\n","val Loss: 0.3217 Acc: 0.9289\n","\n","Epoch 53/99\n","----------\n","train Loss: 0.3307 Acc: 0.9211\n","val Loss: 0.3227 Acc: 0.9289\n","\n","Epoch 54/99\n","----------\n","train Loss: 0.3262 Acc: 0.9230\n","val Loss: 0.3220 Acc: 0.9289\n","\n","Epoch 55/99\n","----------\n","train Loss: 0.3322 Acc: 0.9241\n","val Loss: 0.3216 Acc: 0.9289\n","\n","Epoch 56/99\n","----------\n","train Loss: 0.3301 Acc: 0.9225\n","val Loss: 0.3209 Acc: 0.9289\n","\n","Epoch 57/99\n","----------\n","train Loss: 0.3320 Acc: 0.9209\n","val Loss: 0.3222 Acc: 0.9289\n","\n","Epoch 58/99\n","----------\n","train Loss: 0.3284 Acc: 0.9218\n","val Loss: 0.3214 Acc: 0.9289\n","\n","Epoch 59/99\n","----------\n","train Loss: 0.3290 Acc: 0.9237\n","val Loss: 0.3219 Acc: 0.9289\n","\n","Epoch 60/99\n","----------\n","train Loss: 0.3373 Acc: 0.9221\n","val Loss: 0.3224 Acc: 0.9289\n","\n","Epoch 61/99\n","----------\n","train Loss: 0.3307 Acc: 0.9223\n","val Loss: 0.3229 Acc: 0.9289\n","\n","Epoch 62/99\n","----------\n","train Loss: 0.3252 Acc: 0.9228\n","val Loss: 0.3208 Acc: 0.9289\n","\n","Epoch 63/99\n","----------\n","train Loss: 0.3313 Acc: 0.9228\n","val Loss: 0.3204 Acc: 0.9289\n","\n","Epoch 64/99\n","----------\n","train Loss: 0.3307 Acc: 0.9227\n","val Loss: 0.3201 Acc: 0.9289\n","\n","Epoch 65/99\n","----------\n","train Loss: 0.3303 Acc: 0.9213\n","val Loss: 0.3194 Acc: 0.9289\n","\n","Epoch 66/99\n","----------\n","train Loss: 0.3254 Acc: 0.9243\n","val Loss: 0.3199 Acc: 0.9289\n","\n","Epoch 67/99\n","----------\n","train Loss: 0.3283 Acc: 0.9248\n","val Loss: 0.3213 Acc: 0.9289\n","\n","Epoch 68/99\n","----------\n","train Loss: 0.3288 Acc: 0.9235\n","val Loss: 0.3217 Acc: 0.9289\n","\n","Epoch 69/99\n","----------\n","train Loss: 0.3355 Acc: 0.9218\n","val Loss: 0.3209 Acc: 0.9289\n","\n","Epoch 70/99\n","----------\n"]}]},{"cell_type":"markdown","source":["## efficientnet_v2"],"metadata":{"id":"dlIdoXG2CRRI"}},{"cell_type":"code","source":["from tensorflow import keras\n","def tl(category):\n","    # 画像ディレクトリ\n","    IMG_DIR = anomaly_data_path+category\n","    # バッチサイズ\n","    BATCH_SIZE = 300\n","    # VGG16を使用するため以下のサイズに設定\n","    IMAGE_SIZE = (224, 224)\n","    IMAGE_SHAPE = IMAGE_SIZE + (3, )\n","\n","    # 実行毎に同一の結果が得られるようシード値を固定\n","    RANDOM_STATE = 123\n","\n","    # エポック数\n","    EPOCHS = 10\n","\n","    num_classes=3\n","\n","    train_ds = tf.keras.utils.image_dataset_from_directory(\n","    IMG_DIR,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=123,\n","    label_mode='categorical',#categorical_crossentropyの時はcategorical\n","    image_size=IMAGE_SIZE,\n","    batch_size=BATCH_SIZE)\n","\n","    print(train_ds.class_names)\n","\n","    val_ds=tf.keras.utils.image_dataset_from_directory(\n","    IMG_DIR,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    label_mode='categorical',\n","    seed=123,\n","    image_size=IMAGE_SIZE,\n","    batch_size=BATCH_SIZE)\n","\n","    print(train_ds.class_names)\n","\n","    base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(include_top=False, weights=\"imagenet\")\n","    x = base_model.output\n","    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n","    predictions = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n","\n","    # 学習させる部分の指定\n","    layer_names = [l.name for l in base_model.layers]\n","    idx = layer_names.index('block6a_expand_conv')\n","\n","    base_model.trainable = True\n","    for layer in base_model.layers[:idx]:\n","        layer.trainable = False\n","\n","    model = tf.keras.Model(base_model.input, predictions)\n","    #model=keras.models.load_model(os.path.join(f\"{anomaly_root}{category}_3.h5\"))\n","\n","    model.compile(\n","        loss='categorical_crossentropy',\n","        optimizer='adam',\n","        metrics=[\"accuracy\"]\n","    )\n","\n","    for i in range(EPOCHS):\n","        history = model.fit(\n","            train_ds,\n","            validation_data=val_ds,\n","            batch_size=BATCH_SIZE, \n","            epochs=1, \n","            shuffle=True\n","        )\n","        print(history)\n","        print(i)\n","        model.save(f\"{anomaly_root}{category}_{i}.h5\")"],"metadata":{"id":"P8mIrn_Q51wF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for c in [\"区画線\"]:\n","    tl(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ahimVCcuspgJ","outputId":"bfb5075d-bdc2-4148-9bf2-4af53c559c99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 30743 files belonging to 3 classes.\n","Using 24595 files for training.\n","['その他', '補修不要', '要補修']\n","Found 30743 files belonging to 3 classes.\n","Using 24595 files for training.\n","['その他', '補修不要', '要補修']\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n","24274472/24274472 [==============================] - 2s 0us/step\n","55/82 [===================>..........] - ETA: 51:39 - loss: 0.0760 - accuracy: 0.9719"]}]},{"cell_type":"code","source":["for c in [\"道路標識\",\"照明\",\"区画線\"]:\n","    tl(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"id":"tHy47-fxKaoc","outputId":"b17b7cc7-f00b-4532-b5d8-b80a864bd10f","executionInfo":{"status":"error","timestamp":1676529283460,"user_tz":-540,"elapsed":53175,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 4986 files belonging to 3 classes.\n","Using 3989 files for training.\n","['その他', '補修不要', '要補修']\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"]},{"output_type":"stream","name":"stdout","text":["Found 4986 files belonging to 3 classes.\n","Using 3989 files for training.\n","['その他', '補修不要', '要補修']\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","9406464/9406464 [==============================] - 0s 0us/step\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-c969cea24b94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"道路標識\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"照明\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"区画線\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-48c7ef484aa4>\u001b[0m in \u001b[0;36mtl\u001b[0;34m(category)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# 学習させる部分の指定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'block6a_expand_conv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: 'block6a_expand_conv' is not in list"]}]},{"cell_type":"markdown","source":["### base"],"metadata":{"id":"9mV3AKIa6scu"}},{"cell_type":"code","source":["import os\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# 画像ディレクトリ\n","category=\"区画線\"\n","IMG_DIR = anomaly_data_path+category\n","# バッチサイズ\n","BATCH_SIZE = 300\n","# VGG16を使用するため以下のサイズに設定\n","IMAGE_SIZE = (224, 224)\n","IMAGE_SHAPE = IMAGE_SIZE + (3, )\n","\n","# 実行毎に同一の結果が得られるようシード値を固定\n","RANDOM_STATE = 123\n","\n","# エポック数\n","EPOCHS = 2\n","\n","num_classes=2\n","\n","train_ds = tf.keras.utils.image_dataset_from_directory(\n","  IMG_DIR,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  label_mode='categorical',#categorical_crossentropyの時はcategorical\n","  image_size=IMAGE_SIZE,\n","  batch_size=BATCH_SIZE)\n","\n","val_ds=tf.keras.utils.image_dataset_from_directory(\n","  IMG_DIR,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  label_mode='categorical',\n","  seed=123,\n","  image_size=IMAGE_SIZE,\n","  batch_size=BATCH_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZfRzsSJDIDZ","executionInfo":{"status":"ok","timestamp":1676426972614,"user_tz":-540,"elapsed":43525,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"0b80d3a6-8a50-4dc6-d5ac-222de1e3fcae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 24872 files belonging to 2 classes.\n","Using 19898 files for training.\n","Found 24872 files belonging to 2 classes.\n","Using 19898 files for training.\n"]}]},{"cell_type":"code","source":["base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False, weights=\"imagenet\")\n","x = base_model.output\n","x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","x = tf.keras.layers.Dense(1024, activation='relu')(x)\n","predictions = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n","\n","# 学習させる部分の指定\n","layer_names = [l.name for l in base_model.layers]\n","idx = layer_names.index('block6a_expand_conv')\n","\n","base_model.trainable = True\n","for layer in base_model.layers[:idx]:\n","   layer.trainable = False\n","\n","model = tf.keras.Model(base_model.input, predictions)\n","\n","model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=[\"accuracy\"]\n",")"],"metadata":{"id":"iMNQl-oTC8c3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    batch_size=BATCH_SIZE, \n","    epochs=EPOCHS, \n","    shuffle=True\n",")"],"metadata":{"id":"0QXZSgpkMznZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676426370185,"user_tz":-540,"elapsed":317638,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"c38284d9-4b07-49a5-cd45-bc8744954fe2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","67/67 [==============================] - 176s 2s/step - loss: 0.3028 - accuracy: 0.8664 - val_loss: 0.1534 - val_accuracy: 0.9417\n","Epoch 2/2\n","67/67 [==============================] - 97s 1s/step - loss: 0.1567 - accuracy: 0.9353 - val_loss: 0.0937 - val_accuracy: 0.9628\n"]}]},{"cell_type":"code","source":["model.save(f\"model_{category}.h5\")"],"metadata":{"id":"Y5-DY-kdXp_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow import keras\n","model=keras.models.load_model(f\"model_{category}.h5\")"],"metadata":{"id":"xC8pPxg911w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    batch_size=BATCH_SIZE, \n","    epochs=1, \n","    shuffle=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QPUn20BpzUk7","executionInfo":{"status":"ok","timestamp":1676427769143,"user_tz":-540,"elapsed":142458,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"39a03c5f-27f4-44d3-a770-7c4b06ec6813"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["67/67 [==============================] - 96s 1s/step - loss: 0.0755 - accuracy: 0.9718 - val_loss: 0.0403 - val_accuracy: 0.9861\n"]}]},{"cell_type":"code","source":["model.save(f\"model_{category}_4_.h5\")"],"metadata":{"id":"DN00KprO1DLC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    batch_size=BATCH_SIZE, \n","    epochs=1, \n","    shuffle=True\n",")\n","model.save(f\"model_{category}_5_.h5\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Crqz-R1g4oaY","executionInfo":{"status":"ok","timestamp":1676427976548,"user_tz":-540,"elapsed":143373,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"c0fa5128-061e-4c3c-8f80-3f18210e75ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["67/67 [==============================] - 96s 1s/step - loss: 0.0538 - accuracy: 0.9799 - val_loss: 0.0225 - val_accuracy: 0.9928\n"]}]},{"cell_type":"markdown","source":["## CutPaste"],"metadata":{"id":"sMGzwZbhCNkE"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/signate/ワールドＡＩ/anomaly\n","!git clone https://github.com/LilitYolyan/CutPaste.git\n","%cd CutPaste\n","!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnv-0KmHVHMG","executionInfo":{"status":"ok","timestamp":1676353160630,"user_tz":-540,"elapsed":118262,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"7a382fa2-df64-4eb1-f647-c16afafc3cf0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/signate/ワールドＡＩ/anomaly\n","Cloning into 'CutPaste'...\n","remote: Enumerating objects: 614, done.\u001b[K\n","remote: Counting objects: 100% (8/8), done.\u001b[K\n","remote: Compressing objects: 100% (8/8), done.\u001b[K\n","remote: Total 614 (delta 0), reused 0 (delta 0), pack-reused 606\u001b[K\n","Receiving objects: 100% (614/614), 1.85 MiB | 9.90 MiB/s, done.\n","Resolving deltas: 100% (335/335), done.\n","/content/drive/MyDrive/signate/ワールドＡＩ/anomaly/CutPaste\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting matplotlib==3.5.1\n","  Downloading matplotlib-3.5.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy==1.21.2\n","  Downloading numpy-1.21.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Pillow==8.4.0\n","  Downloading Pillow-8.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch_lightning==1.5.7\n","  Downloading pytorch_lightning-1.5.7-py3-none-any.whl (526 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.2/526.2 KB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit_learn==1.0.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (1.0.2)\n","Collecting torch==1.10.1\n","  Downloading torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.11.2\n","  Downloading torchvision-0.11.2-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tqdm==4.62.3\n","  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fonttools>=4.22.0\n","  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 KB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 1)) (0.11.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 1)) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 1)) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 1)) (2.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 1)) (23.0)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (2023.1.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (2.11.2)\n","Collecting future>=0.17.1\n","  Downloading future-0.18.3.tar.gz (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 KB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (6.0)\n","Collecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyDeprecate==0.3.1\n","  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (4.4.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.0.2->-r requirements.txt (line 5)) (1.7.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.0.2->-r requirements.txt (line 5)) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.0.2->-r requirements.txt (line 5)) (3.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (2.25.1)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (3.8.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.1->-r requirements.txt (line 1)) (1.15.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (57.4.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (0.38.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (3.4.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (1.0.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (0.6.1)\n","Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (3.19.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (2.16.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (0.4.6)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (1.51.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (1.4.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (1.8.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (1.3.3)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (2.1.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (22.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (1.8.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (6.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (3.12.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.5.7->-r requirements.txt (line 4)) (3.2.2)\n","Building wheels for collected packages: future\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492036 sha256=50d1c4b5f25188aa875cd8c4a78aa550e1aec347bdeeb2722b1c42c3f80352f7\n","  Stored in directory: /root/.cache/pip/wheels/a0/0b/ee/e6994fadb42c1354dcccb139b0bf2795271bddfe6253ccdf11\n","Successfully built future\n","Installing collected packages: tqdm, torch, pyDeprecate, Pillow, numpy, future, fonttools, torchvision, torchmetrics, matplotlib, pytorch_lightning\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.64.1\n","    Uninstalling tqdm-4.64.1:\n","      Successfully uninstalled tqdm-4.64.1\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.1+cu116\n","    Uninstalling torch-1.13.1+cu116:\n","      Successfully uninstalled torch-1.13.1+cu116\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.1+cu116\n","    Uninstalling torchvision-0.14.1+cu116:\n","      Successfully uninstalled torchvision-0.14.1+cu116\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.2.2\n","    Uninstalling matplotlib-3.2.2:\n","      Successfully uninstalled matplotlib-3.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\n","torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Pillow-8.4.0 fonttools-4.38.0 future-0.18.3 matplotlib-3.5.1 numpy-1.21.2 pyDeprecate-0.3.1 pytorch_lightning-1.5.7 torch-1.10.1 torchmetrics-0.11.1 torchvision-0.11.2 tqdm-4.62.3\n"]}]},{"cell_type":"code","source":["!pip install torch==1.13.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sWyU3XR25eVQ","executionInfo":{"status":"ok","timestamp":1676360998900,"user_tz":-540,"elapsed":53564,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"9238663f-fdac-4939-8482-279a9102cccf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.13.1\n","  Using cached torch-1.13.1-cp38-cp38-manylinux1_x86_64.whl (887.4 MB)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1) (11.10.3.66)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1) (4.4.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1) (8.5.0.96)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.38.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (57.4.0)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.10.1\n","    Uninstalling torch-1.10.1:\n","      Successfully uninstalled torch-1.10.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.11.2 requires torch==1.10.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.13.1\n"]}]},{"cell_type":"code","source":["!python train.py --dataset_path /content/drive/MyDrive/signate/ワールドＡＩ/anomaly/anomaly_data/区画線 --num_class 2"],"metadata":{"id":"Yhr7R6dgXQ98","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676361047986,"user_tz":-540,"elapsed":4933,"user":{"displayName":"須崎涼","userId":"17559993123309156265"}},"outputId":"3b05213e-a55b-4711-9ae1-89ef316902ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.8/dist-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /usr/local/lib/python3.8/dist-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n","  warn(f\"Failed to load image Python extension: {e}\")\n","Traceback (most recent call last):\n","  File \"train.py\", line 5, in <module>\n","    import pytorch_lightning as pl\n","  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/__init__.py\", line 20, in <module>\n","    from pytorch_lightning.callbacks import Callback  # noqa: E402\n","  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/__init__.py\", line 14, in <module>\n","    from pytorch_lightning.callbacks.base import Callback\n","  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/base.py\", line 26, in <module>\n","    from pytorch_lightning.utilities.types import STEP_OUTPUT\n","  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/__init__.py\", line 18, in <module>\n","    from pytorch_lightning.utilities.apply_func import move_data_to_device  # noqa: F401\n","  File \"/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/apply_func.py\", line 30, in <module>\n","    from torchtext.legacy.data import Batch\n","ModuleNotFoundError: No module named 'torchtext.legacy'\n"]}]}]}